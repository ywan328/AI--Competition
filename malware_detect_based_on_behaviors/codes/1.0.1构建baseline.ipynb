{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务目标--构建baseline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "baseline的思路\n",
    "特征工程思路：\n",
    "* 提取字段的个数 count()\n",
    "* 唯一值个数 nunique() \n",
    "* 统计特征 min, max, mean, std\n",
    "\n",
    "模型思路：（保存到竞赛文档，用于之后快速构建baseline）\n",
    "* lgb祖传参数 （0.73）\n",
    "* xgb祖传参数 （0.68）\n",
    "* nn直桶网络\n",
    "* 简单加权融合\n",
    "\n",
    "\n",
    "提交成绩：\n",
    "lgb全量0.718509\n",
    "xgb全量0.674511\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调包区\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:20.586720Z",
     "start_time": "2021-03-03T04:36:20.105712Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "# 警告忽略\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# matplotlib字体设置\n",
    "plt.rcParams[\"font.family\"] = \"Songti SC\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# matplotlib警告忽略\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# 观看Dataframe长度\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 浮点数位长度\n",
    "pd.set_option('display.precision',5)\n",
    "\n",
    "# 显示多个结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:21.566046Z",
     "start_time": "2021-03-03T04:36:20.588548Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,KFold\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score,auc,f1_score\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_log_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:23.058973Z",
     "start_time": "2021-03-03T04:36:21.568283Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D , GlobalAveragePooling1D,MaxPool1D\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD # 优化方法\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:23.068000Z",
     "start_time": "2021-03-03T04:36:23.061048Z"
    }
   },
   "outputs": [],
   "source": [
    "def lgb_logloss(preds,data):\n",
    "    labels_ = data.get_label()             \n",
    "    classes_ = np.unique(labels_) \n",
    "    preds_prob = []\n",
    "    for i in range(len(classes_)):\n",
    "        preds_prob.append(preds[i*len(labels_):(i+1) * len(labels_)] )\n",
    "        \n",
    "    preds_prob_ = np.vstack(preds_prob) \n",
    "    \n",
    "    loss = []\n",
    "    for i in range(preds_prob_.shape[1]):     # 样本个数\n",
    "        sum_ = 0\n",
    "        for j in range(preds_prob_.shape[0]): #类别个数\n",
    "            pred = preds_prob_[j,i]           # 第i个样本预测为第j类的概率\n",
    "            if  j == labels_[i]:\n",
    "                sum_ += np.log(pred)\n",
    "            else:\n",
    "                sum_ += np.log(1 - pred)\n",
    "        loss.append(sum_)       \n",
    "    return 'loss is: ',-1 * (np.sum(loss) / preds_prob_.shape[1]),False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:37:17.069126Z",
     "start_time": "2021-03-03T04:36:23.069510Z"
    }
   },
   "outputs": [],
   "source": [
    "#path = './sampledata' # 打开样本数据\n",
    "path = './fulldata' # 打开全量数据\n",
    "\n",
    "train_data = pd.read_csv(f'{path}/security_train.csv')\n",
    "test_data = pd.read_csv(f'{path}/security_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标签编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:38:28.861218Z",
     "start_time": "2021-03-03T04:37:17.070985Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标签编码\n",
    "le = LabelEncoder()\n",
    "data['api'] = le.fit_transform(data['api'])\n",
    "\n",
    "# 数据集拆分\n",
    "train_data = data[data['label'].notnull()]\n",
    "test_data = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成特征文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:38:43.441332Z",
     "start_time": "2021-03-03T04:38:28.863048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>api</th>\n",
       "      <th>tid</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8065</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89620181</th>\n",
       "      <td>13883</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89798402</th>\n",
       "      <td>13884</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89799721</th>\n",
       "      <td>13885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89800754</th>\n",
       "      <td>13886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89806070</th>\n",
       "      <td>13887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13887 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  label  api   tid  index\n",
       "0               1    5.0  135  2488      0\n",
       "6786            2    2.0   95  2320      0\n",
       "7602            3    0.0  151  2208      0\n",
       "8065            4    0.0   95  2284      0\n",
       "10111           5    0.0  249  2500      0\n",
       "...           ...    ...  ...   ...    ...\n",
       "89620181    13883    2.0   95   100      0\n",
       "89798402    13884    5.0   95  2592      0\n",
       "89799721    13885    0.0  151  2240      0\n",
       "89800754    13886    1.0   95  2324      0\n",
       "89806070    13887    2.0  135  2336      0\n",
       "\n",
       "[13887 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_feature = ['file_id','label','api','tid','index']\n",
    "\n",
    "train_fe = train_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "test_fe = test_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "train_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建统计值特征（count,nunique,min、max、sum、mean、median）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:41:29.407563Z",
     "start_time": "2021-03-03T04:38:43.443816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到统计值特征（count,nunique,min、max、sum、mean、median）\n",
    "def creat_stats_feature(tem_df):\n",
    "    # 以file_id 为单位聚合，统计每一个file_id下的情况\n",
    "    group_df = tem_df.groupby('file_id')\n",
    "\n",
    "    # 得到 count,nunique,min、max、sum、mean、median的统计值\n",
    "    stats_dict = {\n",
    "        \"api\": ['count', 'nunique', 'min', 'max', 'mean', 'median','std'], \n",
    "        \"tid\":['count', 'nunique', 'min', 'max', 'mean', 'median','std'], \n",
    "        \"index\":['count', 'nunique', 'min', 'max', 'mean', 'median','std']}\n",
    "\n",
    "    stats_df = group_df.agg(stats_dict)\n",
    "    stats_df.columns = [\"_\".join(tup) for tup in stats_df.columns]\n",
    "    stats_df = stats_df.reset_index()\n",
    "    return(stats_df)\n",
    "\n",
    "\n",
    "\n",
    "# 构建训练集的特征\n",
    "stats_df = creat_stats_feature(train_data)\n",
    "train_fe = train_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "# 构建测试集的特征\n",
    "stats_df = creat_stats_feature(test_data)\n",
    "test_fe = test_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "# 回收内存\n",
    "del stats_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:41:30.171646Z",
     "start_time": "2021-03-03T04:41:29.409389Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_fe,test_fe],axis=0)\n",
    "\n",
    "\n",
    "# 进行标准化\n",
    "de_ss_list = ['file_id','label']\n",
    "ss_list = [i for i in data.columns if i not in de_ss_list]\n",
    "\n",
    "ss = StandardScaler()\n",
    "data[ss_list] = ss.fit_transform(data[ss_list])\n",
    "\n",
    "\n",
    "# 数据集拆分\n",
    "train_fe = data[data['label'].notnull()]\n",
    "test_fe = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:41:30.190013Z",
     "start_time": "2021-03-03T04:41:30.173504Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe.to_pickle(f'{path}/train_fe_baseline.pkl')\n",
    "test_fe.to_pickle(f'{path}/test_fe_baseline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:41:30.200688Z",
     "start_time": "2021-03-03T04:41:30.191891Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe = pd.read_pickle(f'{path}/train_fe_baseline.pkl')\n",
    "test_fe = pd.read_pickle(f'{path}/test_fe_baseline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:41:30.204880Z",
     "start_time": "2021-03-03T04:41:30.202385Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 进行数据切分\n",
    "# file_id_list = train_fe['file_id'].unique()\n",
    "\n",
    "# train_id,vaild_id = train_test_split(file_id_list,test_size=0.2)\n",
    "# train_df = train_fe[train_fe['file_id'].isin(train_id)]\n",
    "# vaild_df = train_fe[train_fe['file_id'].isin(vaild_id)]\n",
    "\n",
    "\n",
    "# # 得到训练数据集与提交数据集\n",
    "# train_x,train_y = train_df.drop('label',axis=1),train_df['label']\n",
    "# vaild_x,vaild_y = vaild_df.drop('label',axis=1),vaild_df['label']\n",
    "# sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:41:30.218216Z",
     "start_time": "2021-03-03T04:41:30.206449Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据划分\n",
    "raw_x,raw_y = train_fe.drop('label',axis=1),train_fe['label']\n",
    "sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n",
    "\n",
    "# 进行数据切分\n",
    "train_x,vaild_x,train_y,vaild_y = train_test_split(raw_x,raw_y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模尝试"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T05:48:29.123292Z",
     "start_time": "2021-03-01T05:48:29.119064Z"
    }
   },
   "source": [
    "模型思路：（保存到竞赛文档，用于之后快速构建baseline）\n",
    "* lgb祖传参数 （0.73）\n",
    "* xgb祖传参数 （0.68）\n",
    "* nn直桶网络\n",
    "* 简单加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:45:06.265091Z",
     "start_time": "2021-03-03T04:41:30.219750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=1, learning_rate=0.005, min_child_samples=3,\n",
       "               n_estimators=2000, objective='multiclass', random_state=2021,\n",
       "               reg_alpha=0.25, reg_lambda=0.25, subsample=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/lgb_baseline.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0}\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "            num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='multiclass',\n",
    "            max_depth=-1, learning_rate=0.005, min_child_samples=3, random_state=2021,\n",
    "            n_estimators=2000, subsample=1, colsample_bytree=1,\n",
    "    **params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='logloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "model.fit(\n",
    "    raw_x, raw_y)\n",
    "\n",
    "\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/lgb_baseline.csv',index=None)\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/lgb_baseline.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('lgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用xgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:49:44.159197Z",
     "start_time": "2021-03-03T04:45:06.266842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:45:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { min_child_samples } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='mlogloss',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.005, max_delta_step=0,\n",
       "              max_depth=9, min_child_samples=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=2000, n_jobs=0,\n",
       "              num_parallel_tree=1, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.5, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/xgb_baseline.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {'tree_method': 'gpu_hist'}\n",
    "\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "            max_depth=9, learning_rate=0.005, n_estimators=2000, \n",
    "            objective='multi:softprob', \n",
    "            subsample=0.8, colsample_bytree=0.8, \n",
    "            min_child_samples=3, eval_metric='mlogloss', reg_lambda=0.5,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='mlogloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "model.fit(raw_x, raw_y)\n",
    "\n",
    "\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "sub_df.to_csv(f'{path}/xgb_baseline.csv',index=None)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/xgb_baseline.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('xgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用神经网络进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:49:44.230456Z",
     "start_time": "2021-03-03T04:49:44.161141Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "\n",
    "#############################################\n",
    "# 数据格式转化\n",
    "\n",
    "cate_train_y = keras.utils.to_categorical(train_y,8)\n",
    "cate_vaild_y = keras.utils.to_categorical(vaild_y,8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "# 搭建模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation='relu', input_shape=[len(train_x.columns)]),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'), \n",
    "    keras.layers.Dense(8, activation='softmax') # 需要改写成sigmoid\n",
    "])\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 配置损失函数，评估指标，优化器\n",
    "model.compile(loss='categorical_crossentropy', metrics=['categorical_crossentropy'], optimizer='adam')\n",
    "\n",
    "\n",
    "# 配置backcall\n",
    "callback = ModelCheckpoint(filepath=\"modelfile/MLP.ckpt\",\n",
    "                           monitor=\"val_categorical_crossentropy\",\n",
    "                           verbose=1,save_best_only=True,\n",
    "                           save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:49:52.217712Z",
     "start_time": "2021-03-03T04:49:44.232188Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "110/112 [============================>.] - ETA: 0s - loss: 30.4835 - categorical_crossentropy: 30.4835\n",
      "Epoch 00001: val_categorical_crossentropy improved from inf to 1.64191, saving model to modelfile/MLP.ckpt\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP.ckpt/assets\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 30.1993 - categorical_crossentropy: 30.1993 - val_loss: 1.6419 - val_categorical_crossentropy: 1.6419\n",
      "Epoch 2/5\n",
      "110/112 [============================>.] - ETA: 0s - loss: 1.6259 - categorical_crossentropy: 1.6259\n",
      "Epoch 00002: val_categorical_crossentropy improved from 1.64191 to 1.63694, saving model to modelfile/MLP.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP.ckpt/assets\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 1.6259 - categorical_crossentropy: 1.6259 - val_loss: 1.6369 - val_categorical_crossentropy: 1.6369\n",
      "Epoch 3/5\n",
      "106/112 [===========================>..] - ETA: 0s - loss: 1.6251 - categorical_crossentropy: 1.6251\n",
      "Epoch 00003: val_categorical_crossentropy did not improve from 1.63694\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 1.6250 - categorical_crossentropy: 1.6250 - val_loss: 1.6372 - val_categorical_crossentropy: 1.6372\n",
      "Epoch 4/5\n",
      "107/112 [===========================>..] - ETA: 0s - loss: 1.6257 - categorical_crossentropy: 1.6257\n",
      "Epoch 00004: val_categorical_crossentropy did not improve from 1.63694\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 1.6245 - categorical_crossentropy: 1.6245 - val_loss: 1.6372 - val_categorical_crossentropy: 1.6372\n",
      "Epoch 5/5\n",
      "108/112 [===========================>..] - ETA: 0s - loss: 1.6223 - categorical_crossentropy: 1.6223\n",
      "Epoch 00005: val_categorical_crossentropy did not improve from 1.63694\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 1.6245 - categorical_crossentropy: 1.6245 - val_loss: 1.6374 - val_categorical_crossentropy: 1.6374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe59acf4890>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "# 训练\n",
    "model.fit(train_x, cate_train_y, \n",
    "          validation_data=(vaild_x, cate_vaild_y), \n",
    "          batch_size=100, epochs=5,\n",
    "          callbacks=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:49:53.945810Z",
     "start_time": "2021-03-03T04:49:52.219938Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 进行预测\n",
    "pred_prob = model.predict(sub_x)\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/mlp_baseline.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通的加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:49:54.202250Z",
     "start_time": "2021-03-03T04:49:53.947582Z"
    }
   },
   "outputs": [],
   "source": [
    "# 打开之前的预测结果proba\n",
    "path = './outdata'\n",
    "lgb_pred = pd.read_csv(f'{path}/lgb.csv').set_index('file_id')\n",
    "xgb_pred = pd.read_csv(f'{path}/xgb.csv').set_index('file_id')\n",
    "mlp_pred = pd.read_csv(f'{path}/mlp.csv').set_index('file_id')\n",
    "\n",
    "# 设置权重\n",
    "lgb_weight = 0.2\n",
    "xgb_weight = 0.7\n",
    "mlp_weight = 0.1\n",
    "\n",
    "# 进行加权融合\n",
    "mix_pred = (lgb_weight*lgb_pred)+(xgb_weight*xgb_pred)+(mlp_weight*mlp_pred)\n",
    "\n",
    "# 保存加权融合后的结果\n",
    "mix_pred.to_csv(f'{path}/mix_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:49:54.485347Z",
     "start_time": "2021-03-03T04:49:54.204183Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7cf75004d821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "sub"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
