{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务目标--使用w2v提取特征"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v3版—提取病毒类型匹配程度\n",
    "特征工程思路：\n",
    "* 意义：用embedding表达各类病毒对api类型\n",
    "* 以label为单位进行聚合，将各自的api embedding取mean与sum\n",
    "* 将api 对应的embedding与 mean or sum 进行预先相似度比较\n",
    "\n",
    "模型思路：\n",
    "* lgb祖传参数\n",
    "* xgb祖传参数\n",
    "* nn直桶网络\n",
    "* 简单加权融合\n",
    "\n",
    "\n",
    "分数：\n",
    "全量lgb 0.624847 (v3 0.625853)\n",
    "全量xgb 0.604009(v3 0.601693)\n",
    "\n",
    "相比v3略有提升！\n",
    "\n",
    "结论：\n",
    "在同一个特征的提纯优化起到的效果，已经变得越来越小了，继续提取仅仅是榨取剩余的价值\n",
    "要想有明显的提升可以考虑在构建新特征上入手。例如 tid,index,或者是api与tid、index之间的交互特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调包区\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:20.076432Z",
     "start_time": "2021-03-03T04:36:19.593438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize() # 初始化该这个b...并行库\n",
    "\n",
    "# 警告忽略\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# matplotlib字体设置\n",
    "plt.rcParams[\"font.family\"] = \"Songti SC\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# matplotlib警告忽略\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# 观看Dataframe长度\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 浮点数位长度\n",
    "pd.set_option('display.precision',5)\n",
    "\n",
    "# 显示多个结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:21.040222Z",
     "start_time": "2021-03-03T04:36:20.078415Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,KFold\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score,auc,f1_score\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_log_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:22.502231Z",
     "start_time": "2021-03-03T04:36:21.042533Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D , GlobalAveragePooling1D,MaxPool1D\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD # 优化方法\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:36:22.669929Z",
     "start_time": "2021-03-03T04:36:22.504438Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:37:16.125101Z",
     "start_time": "2021-03-03T04:36:22.671565Z"
    }
   },
   "outputs": [],
   "source": [
    "#path = './sampledata' # 打开样本数据\n",
    "path = './fulldata' # 打开全量数据\n",
    "\n",
    "train_data = pd.read_csv(f'{path}/security_train.csv')\n",
    "test_data = pd.read_csv(f'{path}/security_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标签编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:38:28.231646Z",
     "start_time": "2021-03-03T04:37:16.126952Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标签编码\n",
    "le = LabelEncoder()\n",
    "data['api'] = le.fit_transform(data['api'])\n",
    "\n",
    "# 数据集拆分\n",
    "train_data = data[data['label'].notnull()]\n",
    "test_data = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建w2v特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:47:34.741899Z",
     "start_time": "2021-03-03T04:38:28.233488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390109d0bf1e4143a7fdb690dbe3c152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='embedding转化进度：', max=301.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def creat_w2v_feature(tem_df,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        w2v_df = pd.read_pickle(f'{path}/w2v_feature.pkl')\n",
    "\n",
    "    else:\n",
    "        #################################################\n",
    "        # 对api进行排序\n",
    "        tem_df = tem_df.sort_values(['api','tid','index'])\n",
    "        tem_df['api'] = tem_df['api'].astype(str) # 转化为str\n",
    "    #     tem_df['api'] = le.inverse_transform(tem_df['api']) # 转化回文字\n",
    "\n",
    "        # 设置进度条\n",
    "        from tqdm.notebook import tqdm\n",
    "        tqdm.pandas(desc=\"句子转化进度：\")\n",
    "\n",
    "        # 生成api句子(去重复)\n",
    "        file_api_sequence = tem_df.groupby('file_id')['api'].apply(lambda x:x.drop_duplicates().tolist())\n",
    "\n",
    "        #################################################\n",
    "        # 进行w2v的训练\n",
    "        model = word2vec.Word2Vec(sentences=file_api_sequence, size=5, window=3, min_count=1)\n",
    "\n",
    "        # 将api信息转化为embedding\n",
    "        tqdm.pandas(desc=\"embedding转化进度：\")\n",
    "        tem_df = tem_df['api'].drop_duplicates().reset_index(drop=True).to_frame() # 去重复\n",
    "        w2v_df = tem_df['api'].progress_apply(lambda x:model.wv[x])\n",
    "        w2v_df = pd.DataFrame(w2v_df.tolist())\n",
    "        w2v_df.columns = [f'api_embedding_{i}' for i in w2v_df.columns]\n",
    "\n",
    "        #################################################\n",
    "        # 合并表格\n",
    "        tem_df['api'] = tem_df['api'].astype(int) # 转化回INT\n",
    "        w2v_df = pd.concat([tem_df,w2v_df],axis=1) \n",
    "        \n",
    "        # 保存特征工程文档\n",
    "        path = './temdata'\n",
    "        w2v_df.to_pickle(f'{path}/w2v_feature.pkl')    \n",
    "    \n",
    "    return(w2v_df)\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标准化\n",
    "w2v_df = creat_w2v_feature(data)\n",
    "\n",
    "train_data = train_data.merge(w2v_df,on='api',how='left')\n",
    "test_data = test_data.merge(w2v_df,on='api',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del w2v_df,data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建病毒api类型与用户当前api的匹配程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:51:27.063976Z",
     "start_time": "2021-03-03T04:47:34.745396Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以优化的地方\n",
    "# embedding名称获取\n",
    "# 进行加权修正\n",
    "\n",
    "\n",
    "# 构建病毒api类型与用户当前api的匹配程度\n",
    "def create_api_virus_sim(train_data,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        api_virus_df = pd.read_pickle(f'{path}/create_api_virus_sim.pkl')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "    ################################################################\n",
    "        tem_df = train_data\n",
    "        # 提取每一个病毒的embedding偏好\n",
    "        virus_eb = tem_df.groupby(['label'])['api_embedding_0','api_embedding_1','api_embedding_2','api_embedding_3','api_embedding_4'].mean()\n",
    "        # 更改名称\n",
    "        virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "        # 提取file所有api的embedding\n",
    "        user_eb_feature = ['file_id','api','api_embedding_0',\n",
    "               'api_embedding_1', 'api_embedding_2', 'api_embedding_3',\n",
    "               'api_embedding_4']\n",
    "        user_eb = tem_df[user_eb_feature].set_index(['file_id','api'])\n",
    "\n",
    "        # 病毒0-7的匹配度\n",
    "        api_virus_list = []\n",
    "        for label in range(virus_eb.shape[0]):\n",
    "            virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "            # 进行余弦相似度的比较\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "            # 生成相似矩阵\n",
    "            similary_array = cosine_similarity(user_eb,virus_embedding)\n",
    "\n",
    "            # 生成相似DataFrame,熟悉的中文化\n",
    "            columns_list = index_list = user_eb.index\n",
    "            similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'api_virus_sim_eb{label}'])\n",
    "\n",
    "            api_virus_list.append(similary_df)\n",
    "\n",
    "        api_virus_df = pd.concat(api_virus_list,axis=1)\n",
    "\n",
    "        # 去重，只输出api与病毒的相似度\n",
    "        api_virus_df = api_virus_df.reset_index().drop('file_id',axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "        # 保存特征文档\n",
    "        path = './temdata'\n",
    "        api_virus_df.to_pickle(f'{path}/create_api_virus_sim.pkl')\n",
    "        \n",
    "    ################################################################\n",
    "    \n",
    "    return api_virus_df\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "# 通过训练集构建病毒、api的相似程度\n",
    "api_virus_df = create_api_virus_sim(train_data)\n",
    "\n",
    "# 与训练集拼接\n",
    "train_data = train_data.merge(api_virus_df,on='api',how='left')\n",
    "\n",
    "# 与测试集拼接\n",
    "test_data = test_data.merge(api_virus_df,on='api',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del api_virus_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成特征文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:52:30.478572Z",
     "start_time": "2021-03-03T04:51:27.069881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>api</th>\n",
       "      <th>tid</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8065</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89620181</th>\n",
       "      <td>13883</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89798402</th>\n",
       "      <td>13884</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89799721</th>\n",
       "      <td>13885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89800754</th>\n",
       "      <td>13886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89806070</th>\n",
       "      <td>13887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13887 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  label  api   tid  index\n",
       "0               1    5.0  135  2488      0\n",
       "6786            2    2.0   95  2320      0\n",
       "7602            3    0.0  151  2208      0\n",
       "8065            4    0.0   95  2284      0\n",
       "10111           5    0.0  249  2500      0\n",
       "...           ...    ...  ...   ...    ...\n",
       "89620181    13883    2.0   95   100      0\n",
       "89798402    13884    5.0   95  2592      0\n",
       "89799721    13885    0.0  151  2240      0\n",
       "89800754    13886    1.0   95  2324      0\n",
       "89806070    13887    2.0  135  2336      0\n",
       "\n",
       "[13887 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_fe = train_data[['file_id','label']].drop_duplicates()\n",
    "# test_fe = test_data[['file_id','label']].drop_duplicates()\n",
    "\n",
    "basic_feature = ['file_id','label','api','tid','index']\n",
    "\n",
    "train_fe = train_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "test_fe = test_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "train_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建文件与病毒的匹配程度特征(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:52:46.768515Z",
     "start_time": "2021-03-03T04:52:30.511850Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建病毒各自的类别（mean）\n",
    "def create_file_virus_sim(tem_df):    \n",
    "    global virus_eb,api_embedding_list    \n",
    "    ################################################################\n",
    "    # 一、构建病毒类型\n",
    "    \n",
    "    # 截取api_embedding的列表名称\n",
    "    if tem_df['label'].isnull().sum()>0:\n",
    "        virus_eb = virus_eb\n",
    "        \n",
    "    else:\n",
    "        api_embedding_list = [i for i in tem_df.columns if 'api_embedding' in i]\n",
    "        # 提取每一个病毒的embedding偏好\n",
    "        virus_eb = tem_df.groupby(['label'])[api_embedding_list].mean()\n",
    "        # 更改名称\n",
    "        virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    # 二、构建file的类型\n",
    "\n",
    "    # 构建file的api类别（mean）\n",
    "    file_eb = tem_df.groupby('file_id')[api_embedding_list].mean()\n",
    "    # 更改名称\n",
    "    file_eb.columns = [f'file_{i}' for i in virus_eb.columns]\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # 病毒0-7的匹配度\n",
    "    file_virus_list = []\n",
    "    for label in range(virus_eb.shape[0]):\n",
    "        virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "        # 进行余弦相似度的比较\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 生成相似矩阵\n",
    "        similary_array = cosine_similarity(file_eb,virus_embedding)\n",
    "\n",
    "        # 生成相似DataFrame,熟悉的中文化\n",
    "        columns_list = index_list = file_eb.index\n",
    "        similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'file_virus_mean_sim_eb{label}'])\n",
    "\n",
    "        file_virus_list.append(similary_df)\n",
    "        \n",
    "    file_virus_df = pd.concat(file_virus_list,axis=1)\n",
    "    return file_virus_df\n",
    "\n",
    "\n",
    "def _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        train_fe = pd.read_pickle(f'{path}/create_file_virus_mean_sim_train.pkl')\n",
    "        test_fe = pd.read_pickle(f'{path}/create_file_virus_mean_sim_test.pkl')\n",
    "    \n",
    "    else:\n",
    "    ###############################################################   \n",
    "        # 给训练集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(train_data)\n",
    "        train_fe = train_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        # 给测试集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(test_data)\n",
    "        test_fe = test_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        \n",
    "        # 保存特征\n",
    "        path = './temdata'\n",
    "        train_fe.to_pickle(f'{path}/create_file_virus_mean_sim_train.pkl')\n",
    "        test_fe.to_pickle(f'{path}/create_file_virus_mean_sim_test.pkl')\n",
    "    \n",
    "    ###############################################################\n",
    "        \n",
    "    return (train_fe,test_fe)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "train_fe,test_fe = _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建文件与病毒的匹配程度特征(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T04:53:03.038129Z",
     "start_time": "2021-03-03T04:52:46.770456Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建病毒各自的类别（mean）\n",
    "def create_file_virus_sim(tem_df):    \n",
    "    global virus_eb,api_embedding_list    \n",
    "    ################################################################\n",
    "    # 一、构建病毒类型\n",
    "    \n",
    "    # 截取api_embedding的列表名称\n",
    "    if tem_df['label'].isnull().sum()>0:\n",
    "        virus_eb = virus_eb\n",
    "        \n",
    "    else:\n",
    "        api_embedding_list = [i for i in tem_df.columns if 'api_embedding' in i]\n",
    "        # 提取每一个病毒的embedding api总和\n",
    "        virus_eb = tem_df.groupby(['label'])[api_embedding_list].sum()\n",
    "        # 更改名称\n",
    "        virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    # 二、构建file的类型\n",
    "\n",
    "    # 构建file的api 总和（sum）\n",
    "    file_eb = tem_df.groupby('file_id')[api_embedding_list].sum()\n",
    "    # 更改名称\n",
    "    file_eb.columns = [f'file_{i}' for i in virus_eb.columns]\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # 病毒0-7的匹配度\n",
    "    file_virus_list = []\n",
    "    for label in range(virus_eb.shape[0]):\n",
    "        virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "        # 进行余弦相似度的比较\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 生成相似矩阵\n",
    "        similary_array = cosine_similarity(file_eb,virus_embedding)\n",
    "\n",
    "        # 生成相似DataFrame,熟悉的中文化\n",
    "        columns_list = index_list = file_eb.index\n",
    "        similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'file_virus_mean_sim_eb{label}'])\n",
    "\n",
    "        file_virus_list.append(similary_df)\n",
    "        \n",
    "    file_virus_df = pd.concat(file_virus_list,axis=1)\n",
    "    return file_virus_df\n",
    "\n",
    "\n",
    "def _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        train_fe = pd.read_pickle(f'{path}/create_file_virus_sum_sim_train.pkl')\n",
    "        test_fe = pd.read_pickle(f'{path}/create_file_virus_sum_sim_test.pkl')\n",
    "    \n",
    "    else:\n",
    "    ###############################################################   \n",
    "        # 给训练集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(train_data)\n",
    "        train_fe = train_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        # 给测试集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(test_data)\n",
    "        test_fe = test_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        \n",
    "        # 保存特征\n",
    "        path = './temdata'\n",
    "        train_fe.to_pickle(f'{path}/create_file_virus_sum_sim_train.pkl')\n",
    "        test_fe.to_pickle(f'{path}/create_file_virus_sum_sim_test.pkl')\n",
    "    \n",
    "    ###############################################################\n",
    "        \n",
    "    return (train_fe,test_fe)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "train_fe,test_fe = _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建统计值特征（count,nunique,min、max、sum、mean、median）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:21.988134Z",
     "start_time": "2021-03-03T04:53:03.040136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到统计值特征（count,nunique,min、max、sum、mean、median）\n",
    "def create_stats_feature(tem_df):\n",
    "    # 以file_id 为单位聚合，统计每一个file_id下的情况\n",
    "    group_df = tem_df.groupby('file_id')\n",
    "\n",
    "    # 得到 count,nunique,min、max、sum、mean、median的统计值\n",
    "    stats_dict = {\n",
    "                'api': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'tid': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'index': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_0': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_1': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_2': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_3': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_4': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb1': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb2': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb3': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb4': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb5': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb6': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb7': ['count', 'nunique', 'min', 'max', 'mean', 'median','std']\n",
    "    }\n",
    "        \n",
    "  \n",
    "    stats_df = group_df.agg(stats_dict)\n",
    "    stats_df.columns = [\"_\".join(tup) for tup in stats_df.columns]\n",
    "    stats_df = stats_df.reset_index()\n",
    "    return(stats_df)\n",
    "\n",
    "\n",
    "\n",
    "# 构建训练集的特征\n",
    "stats_df = create_stats_feature(train_data)\n",
    "train_fe = train_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "# 构建测试集的特征\n",
    "stats_df = create_stats_feature(test_data)\n",
    "test_fe = test_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del stats_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:22.722148Z",
     "start_time": "2021-03-03T05:04:21.989715Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_fe,test_fe],axis=0)\n",
    "\n",
    "\n",
    "# 进行标准化\n",
    "de_ss_list = ['file_id','label']\n",
    "ss_list = [i for i in data.columns if i not in de_ss_list]\n",
    "\n",
    "ss = StandardScaler()\n",
    "data[ss_list] = ss.fit_transform(data[ss_list])\n",
    "\n",
    "\n",
    "# 数据集拆分\n",
    "train_fe = data[data['label'].notnull()]\n",
    "test_fe = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:22.826153Z",
     "start_time": "2021-03-03T05:04:22.723857Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe.to_pickle(f'{path}/train_fe_v4.pkl')\n",
    "test_fe.to_pickle(f'{path}/test_fe_v4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:22.850845Z",
     "start_time": "2021-03-03T05:04:22.828182Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe = pd.read_pickle(f'{path}/train_fe_v4.pkl')\n",
    "test_fe = pd.read_pickle(f'{path}/test_fe_v4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:22.870180Z",
     "start_time": "2021-03-03T05:04:22.852595Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 进行数据切分\n",
    "# file_id_list = train_fe['file_id'].unique()\n",
    "\n",
    "# train_id,vaild_id = train_test_split(file_id_list,test_size=0.2)\n",
    "# train_df = train_fe[train_fe['file_id'].isin(train_id)]\n",
    "# vaild_df = train_fe[train_fe['file_id'].isin(vaild_id)]\n",
    "\n",
    "\n",
    "# # 得到训练数据集与提交数据集\n",
    "# train_x,train_y = train_df.drop('label',axis=1),train_df['label']\n",
    "# vaild_x,vaild_y = vaild_df.drop('label',axis=1),vaild_df['label']\n",
    "# sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:22.895228Z",
     "start_time": "2021-03-03T05:04:22.871758Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据划分\n",
    "raw_x,raw_y = train_fe.drop('label',axis=1),train_fe['label']\n",
    "sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n",
    "\n",
    "# 进行数据切分\n",
    "train_x,vaild_x,train_y,vaild_y = train_test_split(raw_x,raw_y,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:04:22.898979Z",
     "start_time": "2021-03-03T05:04:22.896885Z"
    }
   },
   "outputs": [],
   "source": [
    "# columns_list = ['api_count', 'api_nunique', 'api_mean', 'api_sum',\n",
    "#        'api_max', 'api_min', 'api_median', 'tid_count', 'tid_nunique',\n",
    "#        'tid_mean', 'tid_sum', 'tid_max', 'tid_min', 'tid_median',\n",
    "#        'index_count', 'index_nunique', 'index_mean', 'index_sum', 'index_max',\n",
    "#        'index_min', 'index_median']\n",
    "# train_x = train_x[columns_list]\n",
    "# vaild_x = vaild_x[columns_list]\n",
    "# sub_x = sub_x[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模尝试"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T05:48:29.123292Z",
     "start_time": "2021-03-01T05:48:29.119064Z"
    }
   },
   "source": [
    "模型思路：（保存到竞赛文档，用于之后快速构建baseline）\n",
    "* lgb祖传参数 （0.73）\n",
    "* xgb祖传参数 （0.68）\n",
    "* nn直桶网络\n",
    "* 简单加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:08:31.053551Z",
     "start_time": "2021-03-03T05:04:22.900428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=1, learning_rate=0.005, min_child_samples=3,\n",
       "               n_estimators=2000, objective='multiclass', random_state=2021,\n",
       "               reg_alpha=0.25, reg_lambda=0.25, subsample=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/lgb_v4.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0}\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "            num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='multiclass',\n",
    "            max_depth=-1, learning_rate=0.005, min_child_samples=3, random_state=2021,\n",
    "            n_estimators=2000, subsample=1, colsample_bytree=1,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='logloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/lgb_v4.csv',index=None)\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/lgb_v4.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('lgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用xgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:12:32.357338Z",
     "start_time": "2021-03-03T05:08:31.055336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:08:31] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { min_child_samples } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='mlogloss',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.005, max_delta_step=0,\n",
       "              max_depth=9, min_child_samples=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=2000, n_jobs=0,\n",
       "              num_parallel_tree=1, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.5, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/xgb_v4.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {'tree_method': 'gpu_hist'}\n",
    "\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "            max_depth=9, learning_rate=0.005, n_estimators=2000, \n",
    "            objective='multi:softprob', \n",
    "            subsample=0.8, colsample_bytree=0.8, \n",
    "            min_child_samples=3, eval_metric='mlogloss', reg_lambda=0.5,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='mlogloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "sub_df.to_csv(f'{path}/xgb_v4.csv',index=None)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/xgb_v4.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('xgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用神经网络进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:12:32.431326Z",
     "start_time": "2021-03-03T05:12:32.360509Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "\n",
    "#############################################\n",
    "# 数据格式转化\n",
    "\n",
    "cate_train_y = keras.utils.to_categorical(train_y,8)\n",
    "cate_vaild_y = keras.utils.to_categorical(vaild_y,8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "# 搭建模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation='relu', input_shape=[len(train_x.columns)]),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'), \n",
    "    keras.layers.Dense(8, activation='softmax') # 需要改写成sigmoid\n",
    "])\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 配置损失函数，评估指标，优化器\n",
    "model.compile(loss='categorical_crossentropy', metrics=['categorical_crossentropy'], optimizer='adam')\n",
    "\n",
    "\n",
    "# 配置backcall\n",
    "callback = ModelCheckpoint(filepath=\"modelfile/MLP_v4.ckpt\",\n",
    "                           monitor=\"val_categorical_crossentropy\",\n",
    "                           verbose=1,save_best_only=True,\n",
    "                           save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:12:41.195530Z",
     "start_time": "2021-03-03T05:12:32.433120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "111/112 [============================>.] - ETA: 0s - loss: 24.7790 - categorical_crossentropy: 24.7790\n",
      "Epoch 00001: val_categorical_crossentropy improved from inf to 1.62144, saving model to modelfile/MLP_v4.ckpt\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v4.ckpt/assets\n",
      "112/112 [==============================] - 2s 19ms/step - loss: 24.7605 - categorical_crossentropy: 24.7605 - val_loss: 1.6214 - val_categorical_crossentropy: 1.6214\n",
      "Epoch 2/5\n",
      "107/112 [===========================>..] - ETA: 0s - loss: 1.6331 - categorical_crossentropy: 1.6331\n",
      "Epoch 00002: val_categorical_crossentropy improved from 1.62144 to 1.61115, saving model to modelfile/MLP_v4.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v4.ckpt/assets\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 1.6341 - categorical_crossentropy: 1.6341 - val_loss: 1.6112 - val_categorical_crossentropy: 1.6112\n",
      "Epoch 3/5\n",
      "105/112 [===========================>..] - ETA: 0s - loss: 1.6312 - categorical_crossentropy: 1.6312\n",
      "Epoch 00003: val_categorical_crossentropy did not improve from 1.61115\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 1.6313 - categorical_crossentropy: 1.6313 - val_loss: 1.6116 - val_categorical_crossentropy: 1.6116\n",
      "Epoch 4/5\n",
      "104/112 [==========================>...] - ETA: 0s - loss: 1.6293 - categorical_crossentropy: 1.6293\n",
      "Epoch 00004: val_categorical_crossentropy improved from 1.61115 to 1.61051, saving model to modelfile/MLP_v4.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v4.ckpt/assets\n",
      "112/112 [==============================] - 1s 13ms/step - loss: 1.6311 - categorical_crossentropy: 1.6311 - val_loss: 1.6105 - val_categorical_crossentropy: 1.6105\n",
      "Epoch 5/5\n",
      "109/112 [============================>.] - ETA: 0s - loss: 1.6315 - categorical_crossentropy: 1.6315\n",
      "Epoch 00005: val_categorical_crossentropy did not improve from 1.61051\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 1.6308 - categorical_crossentropy: 1.6308 - val_loss: 1.6107 - val_categorical_crossentropy: 1.6107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb621a69c90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "# 训练\n",
    "model.fit(train_x, cate_train_y, \n",
    "          validation_data=(vaild_x, cate_vaild_y), \n",
    "          batch_size=100, epochs=5,\n",
    "          callbacks=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:12:42.423650Z",
     "start_time": "2021-03-03T05:12:41.197959Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 进行预测\n",
    "pred_prob = model.predict(sub_x)\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/mlp_v3.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通的加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T05:12:42.797867Z",
     "start_time": "2021-03-03T05:12:42.425553Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./outdata/mlp_v4.csv does not exist: './outdata/mlp_v4.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4bb04d4f3a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/lgb_v4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/xgb_v4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmlp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/mlp_v4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 设置权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./outdata/mlp_v4.csv does not exist: './outdata/mlp_v4.csv'"
     ]
    }
   ],
   "source": [
    "# 打开之前的预测结果proba\n",
    "path = './outdata'\n",
    "lgb_pred = pd.read_csv(f'{path}/lgb_v4.csv').set_index('file_id')\n",
    "xgb_pred = pd.read_csv(f'{path}/xgb_v4.csv').set_index('file_id')\n",
    "mlp_pred = pd.read_csv(f'{path}/mlp_v4.csv').set_index('file_id')\n",
    "\n",
    "# 设置权重\n",
    "lgb_weight = 0.2\n",
    "xgb_weight = 0.7\n",
    "mlp_weight = 0.1\n",
    "\n",
    "# 进行加权融合\n",
    "mix_pred = (lgb_weight*lgb_pred)+(xgb_weight*xgb_pred)+(mlp_weight*mlp_pred)\n",
    "\n",
    "# 保存加权融合后的结果\n",
    "mix_pred.to_csv(f'{path}/mix_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
