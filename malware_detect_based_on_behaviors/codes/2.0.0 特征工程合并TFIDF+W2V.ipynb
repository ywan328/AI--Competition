{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务目标--TFIDF+W2V"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "特征工程思路\n",
    "* 统计值+TFIDF+W2V\n",
    "\n",
    "模型思路：\n",
    "* lgb祖传参数\n",
    "* xgb祖传参数\n",
    "* 简单加权融合\n",
    "\n",
    "提交成绩：\n",
    "lgb全量：0.522450（0.524398）\n",
    "xgb全量：0.494087（0.497963）\n",
    "\n",
    "相比起TFIDF版本，加上W2V因素之后，效果只有略微提高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调包区\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:20.740175Z",
     "start_time": "2021-03-10T00:42:20.080636Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "# 警告忽略\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# matplotlib字体设置\n",
    "plt.rcParams[\"font.family\"] = \"Songti SC\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# matplotlib警告忽略\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# 观看Dataframe长度\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 浮点数位长度\n",
    "pd.set_option('display.precision',5)\n",
    "\n",
    "# 显示多个结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:22.503047Z",
     "start_time": "2021-03-10T00:42:20.742435Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,KFold\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score,auc,f1_score\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_log_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:25.678509Z",
     "start_time": "2021-03-10T00:42:22.505770Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D , GlobalAveragePooling1D,MaxPool1D\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD # 优化方法\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:26.178146Z",
     "start_time": "2021-03-10T00:42:25.680989Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标签编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:27.329166Z",
     "start_time": "2021-03-10T00:42:26.180080Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './sampledata' # 打开样本数据\n",
    "#path = './fulldata' # 打开全量数据\n",
    "\n",
    "train_data = pd.read_csv(f'{path}/security_train.csv')\n",
    "test_data = pd.read_csv(f'{path}/security_test.csv')\n",
    "\n",
    "test_data['label']=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程（全量级特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:28.510600Z",
     "start_time": "2021-03-10T00:42:27.332027Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标签编码\n",
    "le = LabelEncoder()\n",
    "data['api'] = le.fit_transform(data['api'])\n",
    "\n",
    "# 数据集拆分\n",
    "train_data = data[data['label'].notnull()]\n",
    "test_data = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建w2v特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:45:50.569714Z",
     "start_time": "2021-03-10T00:45:46.161378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6bc8c5b22f48f2bfdcde82a5e638be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='embedding转化进度：', max=255.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'AssignProcessToJobObject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mcreat_w2v_feature\u001b[0;34m(tem_df, cache)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5547\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5548\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     ) -> \"BlockManager\":\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     def convert(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'AssignProcessToJobObject'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def creat_w2v_feature(tem_df,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        w2v_df = pd.read_pickle(f'{path}/w2v_feature.pkl')\n",
    "\n",
    "    else:\n",
    "        #################################################\n",
    "        # 对api进行排序\n",
    "        tem_df = tem_df.sort_values(['api','tid','index'])\n",
    "#         tem_df['api'] = tem_df['api'].astype(str) # 转化为str\n",
    "#         tem_df['api'] = le.inverse_transform(tem_df['api']) # 转化回文字\n",
    "\n",
    "        # 设置进度条\n",
    "        from tqdm.notebook import tqdm\n",
    "        tqdm.pandas(desc=\"句子转化进度：\")\n",
    "\n",
    "        # 生成api句子\n",
    "        file_api_sequence = tem_df.groupby('file_id')['api'].apply(lambda x:x.tolist())\n",
    "\n",
    "        #################################################\n",
    "        # 进行w2v的训练\n",
    "        model = word2vec.Word2Vec(sentences=file_api_sequence, size=5, window=3, min_count=1)\n",
    "\n",
    "        # 将api信息转化为embedding\n",
    "        tqdm.pandas(desc=\"embedding转化进度：\") \n",
    "        tem_df = tem_df['api'].drop_duplicates().reset_index(drop=True).to_frame() # 去重复\n",
    "\n",
    "        w2v_df = tem_df['api'].progress_apply(lambda x:model.wv[x])\n",
    "        w2v_df = pd.DataFrame(w2v_df.tolist())\n",
    "        w2v_df.columns = [f'api_embedding_{i}' for i in w2v_df.columns]\n",
    "        #################################################\n",
    "        # 合并表格\n",
    "        #tem_df['api'] = tem_df['api'].astype(int) # 转化回INT\n",
    "        w2v_df = pd.concat([tem_df,w2v_df],axis=1) \n",
    "        \n",
    "        # 保存特征工程文档\n",
    "        path = './temdata'\n",
    "        w2v_df.to_pickle(f'{path}/w2v_feature.pkl')    \n",
    "    \n",
    "    return(w2v_df)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标准化\n",
    "w2v_df = creat_w2v_feature(data)\n",
    "\n",
    "train_data = train_data.merge(w2v_df,on='api',how='left')\n",
    "test_data = test_data.merge(w2v_df,on='api',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del w2v_df,data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建病毒api类型与用户当前api的匹配程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.490875Z",
     "start_time": "2021-03-10T00:42:31.452357Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'api_embedding_3', 'api_embedding_1', 'api_embedding_0', 'api_embedding_4', 'api_embedding_2'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-178c90de5b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# 通过训练集构建病毒、api的相似程度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mapi_virus_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_api_virus_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# 与训练集拼接\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-178c90de5b0c>\u001b[0m in \u001b[0;36mcreate_api_virus_sim\u001b[0;34m(train_data, cache)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtem_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# 提取每一个病毒的embedding偏好\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mvirus_eb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtem_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_embedding_0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'api_embedding_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'api_embedding_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'api_embedding_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'api_embedding_4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 更改名称\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvirus_eb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'virus_{i}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvirus_eb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m             )\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mbad_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columns not found: {str(bad_keys)[1:-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Columns not found: 'api_embedding_3', 'api_embedding_1', 'api_embedding_0', 'api_embedding_4', 'api_embedding_2'\""
     ]
    }
   ],
   "source": [
    "# 可以优化的地方\n",
    "# embedding名称获取\n",
    "# 进行加权修正\n",
    "\n",
    "\n",
    "# 构建病毒api类型与用户当前api的匹配程度\n",
    "def create_api_virus_sim(train_data,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        api_virus_df = pd.read_pickle(f'{path}/create_api_virus_sim.pkl')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "    ################################################################\n",
    "        tem_df = train_data\n",
    "        # 提取每一个病毒的embedding偏好\n",
    "        virus_eb = tem_df.groupby(['label'])['api_embedding_0','api_embedding_1','api_embedding_2','api_embedding_3','api_embedding_4'].mean()\n",
    "        # 更改名称\n",
    "        virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "        # 提取file所有api的embedding\n",
    "        user_eb_feature = ['file_id','api','api_embedding_0',\n",
    "               'api_embedding_1', 'api_embedding_2', 'api_embedding_3',\n",
    "               'api_embedding_4']\n",
    "        user_eb = tem_df[user_eb_feature].set_index(['file_id','api'])\n",
    "\n",
    "        # 病毒0-7的匹配度\n",
    "        api_virus_list = []\n",
    "        for label in range(virus_eb.shape[0]):\n",
    "            virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "            # 进行余弦相似度的比较\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "            # 生成相似矩阵\n",
    "            similary_array = cosine_similarity(user_eb,virus_embedding)\n",
    "\n",
    "            # 生成相似DataFrame,熟悉的中文化\n",
    "            columns_list = index_list = user_eb.index\n",
    "            similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'api_virus_sim_eb{label}'])\n",
    "\n",
    "            api_virus_list.append(similary_df)\n",
    "\n",
    "        api_virus_df = pd.concat(api_virus_list,axis=1)\n",
    "\n",
    "        # 去重，只输出api与病毒的相似度\n",
    "        api_virus_df = api_virus_df.reset_index().drop('file_id',axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "        # 保存特征文档\n",
    "        path = './temdata'\n",
    "        api_virus_df.to_pickle(f'{path}/create_api_virus_sim.pkl')\n",
    "        \n",
    "    ################################################################\n",
    "    \n",
    "    return api_virus_df\n",
    "\n",
    "\n",
    "# 通过训练集构建病毒、api的相似程度\n",
    "api_virus_df = create_api_virus_sim(train_data)\n",
    "\n",
    "# 与训练集拼接\n",
    "train_data = train_data.merge(api_virus_df,on='api',how='left')\n",
    "\n",
    "# 与测试集拼接\n",
    "test_data = test_data.merge(api_virus_df,on='api',how='left')\n",
    "\n",
    "# 回收内存\n",
    "del api_virus_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程（聚合级特征）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建特征表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.495876Z",
     "start_time": "2021-03-10T00:42:22.018Z"
    }
   },
   "outputs": [],
   "source": [
    "basic_feature = ['file_id','label','api','tid','index']\n",
    "\n",
    "train_fe = train_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "test_fe = test_data[basic_feature].drop_duplicates(['file_id','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建统计值特征（count,nunique,min、max、sum、mean、median）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.498433Z",
     "start_time": "2021-03-10T00:42:22.262Z"
    }
   },
   "outputs": [],
   "source": [
    "# 得到统计值特征（count,nunique,min、max、sum、mean、median）\n",
    "def creat_stats_feature(tem_df):\n",
    "    # 以file_id 为单位聚合，统计每一个file_id下的情况\n",
    "    group_df = tem_df.groupby('file_id')\n",
    "\n",
    "    # 得到 count,nunique,min、max、sum、mean、median的统计值\n",
    "    stats_dict = {\n",
    "                'api': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'tid': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'index': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_0': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_1': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_2': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_3': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_4': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb1': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb2': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb3': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb4': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb5': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb6': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_virus_sim_eb7': ['count', 'nunique', 'min', 'max', 'mean', 'median','std']\n",
    "    }\n",
    "        \n",
    "\n",
    "    stats_df = group_df.agg(stats_dict)\n",
    "    stats_df.columns = [\"_\".join(tup) for tup in stats_df.columns]\n",
    "    stats_df = stats_df.reset_index()\n",
    "    return(stats_df)\n",
    "\n",
    "\n",
    "\n",
    "# 构建训练集的特征\n",
    "stats_df = creat_stats_feature(train_data)\n",
    "train_fe = train_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "# 构建测试集的特征\n",
    "stats_df = creat_stats_feature(test_data)\n",
    "test_fe = test_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del stats_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建TFIDF特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.500619Z",
     "start_time": "2021-03-10T00:42:22.522Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# 构建api句子\n",
    "\n",
    "def get_apis(df):\n",
    "    # 以file_id进行聚合\n",
    "    group_df = df.groupby('file_id')\n",
    "    \n",
    "    # 统计每一个file_id下的api的情况\n",
    "    file_api = {}\n",
    "    for file_id,file_group in group_df:\n",
    "        # 以tid、index从小达到的排序，生成每一个file_id下的api句子\n",
    "        result = file_group.sort_values(['tid','index'],ascending=True)\n",
    "        api_sequence = ' '.join(result['api'])\n",
    "        file_api[file_id] = api_sequence\n",
    "    return(file_api)\n",
    "        \n",
    "\n",
    "## 构建TFIDF特征\n",
    "def get_tfidf(tem_df):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,3),min_df=0.1,max_df=0.8)\n",
    "    api_feature = tfidf.fit_transform(tem_df['apis'])\n",
    "\n",
    "    # 转化为DataFrame\n",
    "    df_apis = pd.DataFrame(api_feature.toarray(),columns=tfidf.get_feature_names()).reset_index(drop=True)\n",
    "    return(df_apis)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "#### 构建api句子\n",
    "def creat_tfidf_feature(train_data,test_data,train_fe,test_fe,cache=False):\n",
    "    # 判断是否需要重新构建特征\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        df_fe = pd.read_pickle(f'{path}/tfidf_feature.pkl')\n",
    "        \n",
    "    \n",
    "    else:\n",
    "    ##########################    \n",
    "        # 数据格式转化\n",
    "#         train_data['api'] = train_data['api'].astype(str)\n",
    "#         test_data['api'] = test_data['api'].astype(str)\n",
    "        \n",
    "        train_data['api'] = le.inverse_transform(train_data['api']) # 转化回文字\n",
    "        test_data['api'] = le.inverse_transform(test_data['api']) # 转化回文字\n",
    "    \n",
    "        # 构建训练集\n",
    "        train_api = get_apis(df = train_data)\n",
    "        temp = pd.DataFrame.from_dict(train_api,orient='index',columns=['apis'])\n",
    "        temp = temp.reset_index().rename(columns={'index':'file_id'})\n",
    "        train_fe = train_fe.merge(temp,on='file_id',how='left')\n",
    "\n",
    "        # 构建测试集\n",
    "        test_api = get_apis(df = test_data)\n",
    "        temp = pd.DataFrame.from_dict(test_api,orient='index',columns=['apis'])\n",
    "        temp = temp.reset_index().rename(columns={'index':'file_id'})\n",
    "        test_fe = test_fe.merge(temp,on='file_id',how='left')\n",
    "\n",
    "        #### 构建tfdif特征\n",
    "        df_fe = pd.concat([train_fe,test_fe],axis=0).reset_index(drop=True)\n",
    "\n",
    "        # 得到tfidf特征\n",
    "        tfidf_feature = get_tfidf(df_fe)\n",
    "\n",
    "\n",
    "        # 合并tfidf特征\n",
    "        df_fe = pd.concat([df_fe,tfidf_feature],axis=1).drop(['apis'],axis=1)\n",
    "        \n",
    "        # 保存tfidf特征\n",
    "        path = './temdata'\n",
    "        df_fe.to_pickle(f'{path}/tfidf_feature.pkl')\n",
    "        \n",
    "    \n",
    "    ##########################\n",
    "\n",
    "\n",
    "    # 划分训练集\n",
    "    train_fe = df_fe[df_fe['label'].notnull()]\n",
    "\n",
    "    # 划分测试集\n",
    "    test_fe = df_fe[df_fe['label'].isnull()]\n",
    "\n",
    "    # 回收内存\n",
    "    del tfidf_feature,df_fe\n",
    "    gc.collect()\n",
    "    \n",
    "    return(train_fe,test_fe)\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "train_fe,test_fe = creat_tfidf_feature(train_data,test_data,train_fe,test_fe,cache=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建文件与病毒的匹配程度特征(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.502297Z",
     "start_time": "2021-03-10T00:42:22.770Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建病毒各自的类别（mean）\n",
    "def create_file_virus_sim(tem_df):    \n",
    "    global virus_eb,api_embedding_list    \n",
    "    ################################################################\n",
    "    # 一、构建病毒类型\n",
    "    \n",
    "    # 截取api_embedding的列表名称\n",
    "    if tem_df['label'].isnull().sum()>0:\n",
    "        virus_eb = virus_eb\n",
    "        \n",
    "    else:\n",
    "        api_embedding_list = [i for i in tem_df.columns if 'api_embedding' in i]\n",
    "        # 提取每一个病毒的embedding偏好\n",
    "        virus_eb = tem_df.groupby(['label'])[api_embedding_list].mean()\n",
    "        # 更改名称\n",
    "        virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    # 二、构建file的类型\n",
    "\n",
    "    # 构建file的api类别（mean）\n",
    "    file_eb = tem_df.groupby('file_id')[api_embedding_list].mean()\n",
    "    # 更改名称\n",
    "    file_eb.columns = [f'file_{i}' for i in virus_eb.columns]\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # 病毒0-7的匹配度\n",
    "    file_virus_list = []\n",
    "    for label in range(virus_eb.shape[0]):\n",
    "        virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "        # 进行余弦相似度的比较\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 生成相似矩阵\n",
    "        similary_array = cosine_similarity(file_eb,virus_embedding)\n",
    "\n",
    "        # 生成相似DataFrame,熟悉的中文化\n",
    "        columns_list = index_list = file_eb.index\n",
    "        similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'file_virus_mean_sim_eb{label}'])\n",
    "\n",
    "        file_virus_list.append(similary_df)\n",
    "        \n",
    "    file_virus_df = pd.concat(file_virus_list,axis=1)\n",
    "    return file_virus_df\n",
    "\n",
    "\n",
    "def _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        train_fe = pd.read_pickle(f'{path}/create_file_virus_mean_sim_train.pkl')\n",
    "        test_fe = pd.read_pickle(f'{path}/create_file_virus_mean_sim_test.pkl')\n",
    "    \n",
    "    else:\n",
    "    ###############################################################   \n",
    "        # 给训练集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(train_data)\n",
    "        train_fe = train_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        # 给测试集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(test_data)\n",
    "        test_fe = test_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        \n",
    "        # 保存特征\n",
    "        path = './temdata'\n",
    "        train_fe.to_pickle(f'{path}/create_file_virus_mean_sim_train.pkl')\n",
    "        test_fe.to_pickle(f'{path}/create_file_virus_mean_sim_test.pkl')\n",
    "    \n",
    "    ###############################################################\n",
    "        \n",
    "    return (train_fe,test_fe)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "train_fe,test_fe = _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建文件与病毒的匹配程度特征(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.503796Z",
     "start_time": "2021-03-10T00:42:22.997Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建病毒各自的类别（mean）\n",
    "def create_file_virus_sim(tem_df):    \n",
    "    global virus_eb,api_embedding_list    \n",
    "    ################################################################\n",
    "    # 一、构建病毒类型\n",
    "    \n",
    "    # 截取api_embedding的列表名称\n",
    "    if tem_df['label'].isnull().sum()>0:\n",
    "        virus_eb = virus_eb\n",
    "        \n",
    "    else:\n",
    "        api_embedding_list = [i for i in tem_df.columns if 'api_embedding' in i]\n",
    "        # 提取每一个病毒的embedding api总和\n",
    "        virus_eb = tem_df.groupby(['label'])[api_embedding_list].sum()\n",
    "        # 更改名称\n",
    "        virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "    \n",
    "    ###############################################################\n",
    "    # 二、构建file的类型\n",
    "\n",
    "    # 构建file的api 总和（sum）\n",
    "    file_eb = tem_df.groupby('file_id')[api_embedding_list].sum()\n",
    "    # 更改名称\n",
    "    file_eb.columns = [f'file_{i}' for i in virus_eb.columns]\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # 病毒0-7的匹配度\n",
    "    file_virus_list = []\n",
    "    for label in range(virus_eb.shape[0]):\n",
    "        virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "        # 进行余弦相似度的比较\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 生成相似矩阵\n",
    "        similary_array = cosine_similarity(file_eb,virus_embedding)\n",
    "\n",
    "        # 生成相似DataFrame,熟悉的中文化\n",
    "        columns_list = index_list = file_eb.index\n",
    "        similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'file_virus_sum_sim_eb{label}'])\n",
    "\n",
    "        file_virus_list.append(similary_df)\n",
    "        \n",
    "    file_virus_df = pd.concat(file_virus_list,axis=1)\n",
    "    return file_virus_df\n",
    "\n",
    "\n",
    "def _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False):\n",
    "    if cache==True:\n",
    "        path = './temdata'\n",
    "        train_fe = pd.read_pickle(f'{path}/create_file_virus_sum_sim_train.pkl')\n",
    "        test_fe = pd.read_pickle(f'{path}/create_file_virus_sum_sim_test.pkl')\n",
    "    \n",
    "    else:\n",
    "    ###############################################################   \n",
    "        # 给训练集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(train_data)\n",
    "        train_fe = train_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        # 给测试集构建匹配特征\n",
    "        file_virus_df = create_file_virus_sim(test_data)\n",
    "        test_fe = test_fe.merge(file_virus_df,on='file_id',how='left')\n",
    "        \n",
    "        # 保存特征\n",
    "        path = './temdata'\n",
    "        train_fe.to_pickle(f'{path}/create_file_virus_sum_sim_train.pkl')\n",
    "        test_fe.to_pickle(f'{path}/create_file_virus_sum_sim_test.pkl')\n",
    "    \n",
    "    ###############################################################\n",
    "        \n",
    "    return (train_fe,test_fe)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "train_fe,test_fe = _create_file_virus_sim(train_data,test_data,train_fe,test_fe,cache=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.505627Z",
     "start_time": "2021-03-10T00:42:23.072Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.507909Z",
     "start_time": "2021-03-10T00:42:23.157Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe.to_pickle(f'{path}/train_fe_v2.0.pkl')\n",
    "test_fe.to_pickle(f'{path}/test_fe_v2.0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.509647Z",
     "start_time": "2021-03-10T00:42:23.159Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe = pd.read_pickle(f'{path}/train_fe_v2.0.pkl')\n",
    "test_fe = pd.read_pickle(f'{path}/test_fe_v2.0.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.511514Z",
     "start_time": "2021-03-10T00:42:23.249Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据划分\n",
    "raw_x,raw_y = train_fe.drop('label',axis=1),train_fe['label']\n",
    "sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n",
    "\n",
    "# 进行数据切分\n",
    "train_x,vaild_x,train_y,vaild_y = train_test_split(raw_x,raw_y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模尝试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.513174Z",
     "start_time": "2021-03-10T00:42:23.536Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0}\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "            num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='multiclass',\n",
    "            max_depth=-1, learning_rate=0.005, min_child_samples=3, random_state=2021,\n",
    "            n_estimators=2000, subsample=1, colsample_bytree=1,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='logloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/lgb_v2.0.csv',index=None)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 观察特征重要性\n",
    "feature_list = train_x.columns\n",
    "value = model.feature_importances_.flatten()\n",
    "print(pd.Series(value,index=feature_list).sort_values(ascending=False).to_string())\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/lgb_v2.0.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('lgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用xgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.514922Z",
     "start_time": "2021-03-10T00:42:23.673Z"
    }
   },
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {'tree_method': 'gpu_hist'}\n",
    "\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "            max_depth=9, learning_rate=0.005, n_estimators=2000, \n",
    "            objective='multi:softprob', \n",
    "            subsample=0.8, colsample_bytree=0.8, \n",
    "            min_child_samples=3, eval_metric='mlogloss', reg_lambda=0.5,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='mlogloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "sub_df.to_csv(f'{path}/xgb_v2.0.csv',index=None)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 观察特征重要性\n",
    "feature_list = train_x.columns\n",
    "value = model.feature_importances_.flatten()\n",
    "print(pd.Series(value,index=feature_list).sort_values(ascending=False).to_string())\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/xgb_v2.0.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('xgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通的加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T00:42:31.516913Z",
     "start_time": "2021-03-10T00:42:23.886Z"
    }
   },
   "outputs": [],
   "source": [
    "# 打开之前的预测结果proba\n",
    "path = './outdata'\n",
    "lgb_pred = pd.read_csv(f'{path}/lgb_v2.0.csv').set_index('file_id')\n",
    "xgb_pred = pd.read_csv(f'{path}/xgb_v2.0.csv').set_index('file_id')\n",
    "\n",
    "# 设置权重\n",
    "lgb_weight = 0.4\n",
    "xgb_weight = 0.6\n",
    "mlp_weight = 0.1\n",
    "\n",
    "# 进行加权融合\n",
    "mix_pred = (lgb_weight*lgb_pred)+(xgb_weight*xgb_pred)\n",
    "\n",
    "# 保存加权融合后的结果\n",
    "mix_pred.to_csv(f'{path}/mix_v2.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
