{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务目标--使用w2v提取特征"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v2版—使用w2v提取特征 思路\n",
    "特征工程思路：\n",
    "* 对api 排序 + 构建sequence + w2v\n",
    "\n",
    "模型思路：\n",
    "* lgb祖传参数\n",
    "* xgb祖传参数\n",
    "* nn直桶网络\n",
    "* 简单加权融合\n",
    "\n",
    "分数：\n",
    "全量lgb 0.635024\n",
    "全量xgb 0.612954\n",
    "\n",
    "相比baseline明显提升！！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调包区\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:30:47.637141Z",
     "start_time": "2021-03-03T06:30:44.980202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize() # 初始化该这个b...并行库\n",
    "\n",
    "# 警告忽略\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# matplotlib字体设置\n",
    "plt.rcParams[\"font.family\"] = \"Songti SC\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# matplotlib警告忽略\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# 观看Dataframe长度\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 浮点数位长度\n",
    "pd.set_option('display.precision',5)\n",
    "\n",
    "# 显示多个结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:30:52.113934Z",
     "start_time": "2021-03-03T06:30:47.640160Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,KFold\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score,auc,f1_score\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_log_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:31:01.447248Z",
     "start_time": "2021-03-03T06:30:52.116397Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D , GlobalAveragePooling1D,MaxPool1D\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD # 优化方法\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:31:02.782938Z",
     "start_time": "2021-03-03T06:31:01.449801Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:32:19.779746Z",
     "start_time": "2021-03-03T06:31:02.787372Z"
    }
   },
   "outputs": [],
   "source": [
    "#path = './sampledata' # 打开样本数据\n",
    "path = './fulldata' # 打开全量数据\n",
    "\n",
    "train_data = pd.read_csv(f'{path}/security_train.csv')\n",
    "test_data = pd.read_csv(f'{path}/security_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标签编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:33:59.849820Z",
     "start_time": "2021-03-03T06:32:19.781861Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标签编码\n",
    "le = LabelEncoder()\n",
    "data['api'] = le.fit_transform(data['api'])\n",
    "\n",
    "# 数据集拆分\n",
    "train_data = data[data['label'].notnull()]\n",
    "test_data = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建w2v特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:43:09.810351Z",
     "start_time": "2021-03-03T06:33:59.851859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acd8c61900e463eb28f5b521df678c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='embedding转化进度：', max=301.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creat_w2v_feature(tem_df):\n",
    "    #################################################\n",
    "    # 对api进行排序\n",
    "    tem_df = tem_df.sort_values(['api','tid','index'])\n",
    "    tem_df['api'] = tem_df['api'].astype(str) # 转化为str\n",
    "#     tem_df['api'] = le.inverse_transform(tem_df['api']) # 转化回文字\n",
    "\n",
    "    # 设置进度条\n",
    "    from tqdm.notebook import tqdm\n",
    "    tqdm.pandas(desc=\"句子转化进度：\")\n",
    "\n",
    "    # 生成api句子(去重复)\n",
    "    file_api_sequence = tem_df.groupby('file_id')['api'].apply(lambda x:x.drop_duplicates().tolist())\n",
    "\n",
    "    #################################################\n",
    "    # 进行w2v的训练\n",
    "    model = word2vec.Word2Vec(sentences=file_api_sequence, size=5, window=3, min_count=1)\n",
    "\n",
    "    # 将api信息转化为embedding\n",
    "    tqdm.pandas(desc=\"embedding转化进度：\")\n",
    "    tem_df = tem_df['api'].drop_duplicates().reset_index(drop=True).to_frame() # 去重复\n",
    "    w2v_df = tem_df['api'].progress_apply(lambda x:model.wv[x])\n",
    "    w2v_df = pd.DataFrame(w2v_df.tolist())\n",
    "    w2v_df.columns = [f'api_embedding_{i}' for i in w2v_df.columns]\n",
    "    \n",
    "    #################################################\n",
    "    # 合并表格\n",
    "    tem_df['api'] = tem_df['api'].astype(int) # 转化回INT\n",
    "    w2v_df = pd.concat([tem_df,w2v_df],axis=1) \n",
    "     \n",
    "    \n",
    "    return(w2v_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标准化\n",
    "w2v_df = creat_w2v_feature(data)\n",
    "\n",
    "train_data = train_data.merge(w2v_df,on='api',how='left')\n",
    "test_data = test_data.merge(w2v_df,on='api',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del w2v_df,data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成特征文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:44:04.861336Z",
     "start_time": "2021-03-03T06:43:09.812831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>api</th>\n",
       "      <th>tid</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8065</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89620181</th>\n",
       "      <td>13883</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89798402</th>\n",
       "      <td>13884</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89799721</th>\n",
       "      <td>13885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89800754</th>\n",
       "      <td>13886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89806070</th>\n",
       "      <td>13887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13887 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  label  api   tid  index\n",
       "0               1    5.0  135  2488      0\n",
       "6786            2    2.0   95  2320      0\n",
       "7602            3    0.0  151  2208      0\n",
       "8065            4    0.0   95  2284      0\n",
       "10111           5    0.0  249  2500      0\n",
       "...           ...    ...  ...   ...    ...\n",
       "89620181    13883    2.0   95   100      0\n",
       "89798402    13884    5.0   95  2592      0\n",
       "89799721    13885    0.0  151  2240      0\n",
       "89800754    13886    1.0   95  2324      0\n",
       "89806070    13887    2.0  135  2336      0\n",
       "\n",
       "[13887 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_feature = ['file_id','label','api','tid','index']\n",
    "\n",
    "train_fe = train_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "test_fe = test_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "train_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建统计值特征（count,nunique,min、max、sum、mean、median）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.041146Z",
     "start_time": "2021-03-03T06:44:04.874348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到统计值特征（count,nunique,min、max、sum、mean、median）\n",
    "def create_stats_feature(tem_df):\n",
    "    # 以file_id 为单位聚合，统计每一个file_id下的情况\n",
    "    group_df = tem_df.groupby('file_id')\n",
    "\n",
    "    # 得到 count,nunique,min、max、sum、mean、median的统计值\n",
    "    stats_dict = {\n",
    "                'api': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'tid': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'index': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_0': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_1': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_2': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_3': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_4': ['count', 'nunique', 'min', 'max', 'mean', 'median','std']\n",
    "    }\n",
    "        \n",
    "  \n",
    "    stats_df = group_df.agg(stats_dict)\n",
    "    stats_df.columns = [\"_\".join(tup) for tup in stats_df.columns]\n",
    "    stats_df = stats_df.reset_index()\n",
    "    return(stats_df)\n",
    "\n",
    "\n",
    "\n",
    "# 构建训练集的特征\n",
    "stats_df = create_stats_feature(train_data)\n",
    "train_fe = train_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "# 构建测试集的特征\n",
    "stats_df = create_stats_feature(test_data)\n",
    "test_fe = test_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del stats_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.569479Z",
     "start_time": "2021-03-03T06:52:54.043170Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_fe,test_fe],axis=0)\n",
    "\n",
    "\n",
    "# 进行标准化\n",
    "de_ss_list = ['file_id','label']\n",
    "ss_list = [i for i in data.columns if i not in de_ss_list]\n",
    "\n",
    "ss = StandardScaler()\n",
    "data[ss_list] = ss.fit_transform(data[ss_list])\n",
    "\n",
    "\n",
    "# 数据集拆分\n",
    "train_fe = data[data['label'].notnull()]\n",
    "test_fe = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.603199Z",
     "start_time": "2021-03-03T06:52:54.571500Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe.to_pickle(f'{path}/train_fe_v2.pkl')\n",
    "test_fe.to_pickle(f'{path}/test_fe_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.628252Z",
     "start_time": "2021-03-03T06:52:54.605118Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe = pd.read_pickle(f'{path}/train_fe_v2.pkl')\n",
    "test_fe = pd.read_pickle(f'{path}/test_fe_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.645632Z",
     "start_time": "2021-03-03T06:52:54.642381Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 进行数据切分\n",
    "# file_id_list = train_fe['file_id'].unique()\n",
    "\n",
    "# train_id,vaild_id = train_test_split(file_id_list,test_size=0.2)\n",
    "# train_df = train_fe[train_fe['file_id'].isin(train_id)]\n",
    "# vaild_df = train_fe[train_fe['file_id'].isin(vaild_id)]\n",
    "\n",
    "\n",
    "# # 得到训练数据集与提交数据集\n",
    "# train_x,train_y = train_df.drop('label',axis=1),train_df['label']\n",
    "# vaild_x,vaild_y = vaild_df.drop('label',axis=1),vaild_df['label']\n",
    "# sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.674305Z",
     "start_time": "2021-03-03T06:52:54.647577Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据划分\n",
    "raw_x,raw_y = train_fe.drop('label',axis=1),train_fe['label']\n",
    "sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n",
    "\n",
    "# 进行数据切分\n",
    "train_x,vaild_x,train_y,vaild_y = train_test_split(raw_x,raw_y,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T06:52:54.681763Z",
     "start_time": "2021-03-03T06:52:54.676400Z"
    }
   },
   "outputs": [],
   "source": [
    "# columns_list = ['api_count', 'api_nunique', 'api_mean', 'api_sum',\n",
    "#        'api_max', 'api_min', 'api_median', 'tid_count', 'tid_nunique',\n",
    "#        'tid_mean', 'tid_sum', 'tid_max', 'tid_min', 'tid_median',\n",
    "#        'index_count', 'index_nunique', 'index_mean', 'index_sum', 'index_max',\n",
    "#        'index_min', 'index_median']\n",
    "# train_x = train_x[columns_list]\n",
    "# vaild_x = vaild_x[columns_list]\n",
    "# sub_x = sub_x[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模尝试"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T05:48:29.123292Z",
     "start_time": "2021-03-01T05:48:29.119064Z"
    }
   },
   "source": [
    "模型思路：（保存到竞赛文档，用于之后快速构建baseline）\n",
    "* lgb祖传参数 （0.73）\n",
    "* xgb祖传参数 （0.68）\n",
    "* nn直桶网络\n",
    "* 简单加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:21:48.843392Z",
     "start_time": "2021-03-03T06:52:54.683308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=1, learning_rate=0.005, min_child_samples=3,\n",
       "               n_estimators=2000, objective='multiclass', random_state=2021,\n",
       "               reg_alpha=0.25, reg_lambda=0.25, subsample=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/lgb_v2.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0}\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "            num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='multiclass',\n",
    "            max_depth=-1, learning_rate=0.005, min_child_samples=3, random_state=2021,\n",
    "            n_estimators=2000, subsample=1, colsample_bytree=1,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='logloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/lgb_v2.csv',index=None)\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/lgb_v2.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('lgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用xgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:36:04.867779Z",
     "start_time": "2021-03-03T07:21:48.845881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:21:48] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { min_child_samples } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='mlogloss',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.005, max_delta_step=0,\n",
       "              max_depth=9, min_child_samples=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=2000, n_jobs=0,\n",
       "              num_parallel_tree=1, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.5, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/xgb_v2.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {'tree_method': 'gpu_hist'}\n",
    "\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "            max_depth=9, learning_rate=0.005, n_estimators=2000, \n",
    "            objective='multi:softprob', \n",
    "            subsample=0.8, colsample_bytree=0.8, \n",
    "            min_child_samples=3, eval_metric='mlogloss', reg_lambda=0.5,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='mlogloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "sub_df.to_csv(f'{path}/xgb_v2.csv',index=None)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/xgb_v2.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('xgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用神经网络进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:36:05.385089Z",
     "start_time": "2021-03-03T07:36:04.869824Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "\n",
    "#############################################\n",
    "# 数据格式转化\n",
    "\n",
    "cate_train_y = keras.utils.to_categorical(train_y,8)\n",
    "cate_vaild_y = keras.utils.to_categorical(vaild_y,8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "# 搭建模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation='relu', input_shape=[len(train_x.columns)]),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'), \n",
    "    keras.layers.Dense(8, activation='softmax') # 需要改写成sigmoid\n",
    "])\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 配置损失函数，评估指标，优化器\n",
    "model.compile(loss='categorical_crossentropy', metrics=['categorical_crossentropy'], optimizer='adam')\n",
    "\n",
    "\n",
    "# 配置backcall\n",
    "callback = ModelCheckpoint(filepath=\"modelfile/MLP_v2.ckpt\",\n",
    "                           monitor=\"val_categorical_crossentropy\",\n",
    "                           verbose=1,save_best_only=True,\n",
    "                           save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:36:18.347342Z",
     "start_time": "2021-03-03T07:36:05.387433Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "111/112 [============================>.] - ETA: 0s - loss: 38.0679 - categorical_crossentropy: 38.0679\n",
      "Epoch 00001: val_categorical_crossentropy improved from inf to 1.65047, saving model to modelfile/MLP_v2.ckpt\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v2.ckpt/assets\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 38.0384 - categorical_crossentropy: 38.0384 - val_loss: 1.6505 - val_categorical_crossentropy: 1.6505\n",
      "Epoch 2/5\n",
      "111/112 [============================>.] - ETA: 0s - loss: 1.6271 - categorical_crossentropy: 1.6271\n",
      "Epoch 00002: val_categorical_crossentropy improved from 1.65047 to 1.64003, saving model to modelfile/MLP_v2.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v2.ckpt/assets\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 1.6273 - categorical_crossentropy: 1.6273 - val_loss: 1.6400 - val_categorical_crossentropy: 1.6400\n",
      "Epoch 3/5\n",
      "111/112 [============================>.] - ETA: 0s - loss: 1.6238 - categorical_crossentropy: 1.6238\n",
      "Epoch 00003: val_categorical_crossentropy improved from 1.64003 to 1.63990, saving model to modelfile/MLP_v2.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v2.ckpt/assets\n",
      "112/112 [==============================] - 2s 19ms/step - loss: 1.6240 - categorical_crossentropy: 1.6240 - val_loss: 1.6399 - val_categorical_crossentropy: 1.6399\n",
      "Epoch 4/5\n",
      "110/112 [============================>.] - ETA: 0s - loss: 1.6247 - categorical_crossentropy: 1.6247\n",
      "Epoch 00004: val_categorical_crossentropy improved from 1.63990 to 1.63932, saving model to modelfile/MLP_v2.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v2.ckpt/assets\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 1.6238 - categorical_crossentropy: 1.6238 - val_loss: 1.6393 - val_categorical_crossentropy: 1.6393\n",
      "Epoch 5/5\n",
      "111/112 [============================>.] - ETA: 0s - loss: 1.6232 - categorical_crossentropy: 1.6232\n",
      "Epoch 00005: val_categorical_crossentropy improved from 1.63932 to 1.63910, saving model to modelfile/MLP_v2.ckpt\n",
      "INFO:tensorflow:Assets written to: modelfile/MLP_v2.ckpt/assets\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 1.6236 - categorical_crossentropy: 1.6236 - val_loss: 1.6391 - val_categorical_crossentropy: 1.6391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5f31600790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "# 训练\n",
    "model.fit(train_x, cate_train_y, \n",
    "          validation_data=(vaild_x, cate_vaild_y), \n",
    "          batch_size=100, epochs=5,\n",
    "          callbacks=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:36:19.744574Z",
     "start_time": "2021-03-03T07:36:18.349430Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 进行预测\n",
    "pred_prob = model.predict(sub_x)\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/mlp_v2.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通的加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T07:36:20.166508Z",
     "start_time": "2021-03-03T07:36:19.746854Z"
    }
   },
   "outputs": [],
   "source": [
    "# 打开之前的预测结果proba\n",
    "path = './outdata'\n",
    "lgb_pred = pd.read_csv(f'{path}/lgb_v2.csv').set_index('file_id')\n",
    "xgb_pred = pd.read_csv(f'{path}/xgb_v2.csv').set_index('file_id')\n",
    "mlp_pred = pd.read_csv(f'{path}/mlp_v2.csv').set_index('file_id')\n",
    "\n",
    "# 设置权重\n",
    "lgb_weight = 0.2\n",
    "xgb_weight = 0.7\n",
    "mlp_weight = 0.1\n",
    "\n",
    "# 进行加权融合\n",
    "mix_pred = (lgb_weight*lgb_pred)+(xgb_weight*xgb_pred)+(mlp_weight*mlp_pred)\n",
    "\n",
    "# 保存加权融合后的结果\n",
    "mix_pred.to_csv(f'{path}/mix_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
