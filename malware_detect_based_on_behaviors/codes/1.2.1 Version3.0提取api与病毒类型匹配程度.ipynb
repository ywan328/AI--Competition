{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务目标--使用w2v提取特征"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v3版—提取病毒类型匹配程度\n",
    "特征工程思路：\n",
    "* 意义：用embedding表达各类病毒对api类型\n",
    "* 以label为单位进行聚合，将各自的api embedding取mean与sum\n",
    "* 将api 对应的embedding与 mean or sum 进行预先相似度比较\n",
    "\n",
    "模型思路：\n",
    "* lgb祖传参数\n",
    "* xgb祖传参数\n",
    "* nn直桶网络\n",
    "* 简单加权融合\n",
    "\n",
    "分数：\n",
    "全量lgb:0.625853 （v2 0.635024）\n",
    "全量xgb:0.601693 (v2 0.612954)\n",
    "\n",
    "相比起v2略有提高！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调包区\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:19:13.138500Z",
     "start_time": "2021-03-03T09:19:12.570194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize() # 初始化该这个b...并行库\n",
    "\n",
    "# 警告忽略\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# matplotlib字体设置\n",
    "plt.rcParams[\"font.family\"] = \"Songti SC\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# matplotlib警告忽略\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# 观看Dataframe长度\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 浮点数位长度\n",
    "pd.set_option('display.precision',5)\n",
    "\n",
    "# 显示多个结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # ['all', 'last', 'last_expr', 'none', 'last_expr_or_assign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:19:14.579399Z",
     "start_time": "2021-03-03T09:19:13.140592Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,KFold\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score,auc,f1_score\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_log_error\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:19:19.945555Z",
     "start_time": "2021-03-03T09:19:14.581726Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D , GlobalAveragePooling1D,MaxPool1D\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD # 优化方法\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:19:20.311625Z",
     "start_time": "2021-03-03T09:19:19.947797Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 打开文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:20:13.329151Z",
     "start_time": "2021-03-03T09:19:20.313574Z"
    }
   },
   "outputs": [],
   "source": [
    "# path = './sampledata' # 打开样本数据\n",
    "path = './fulldata' # 打开全量数据\n",
    "\n",
    "train_data = pd.read_csv(f'{path}/security_train.csv')\n",
    "test_data = pd.read_csv(f'{path}/security_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标签编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:21:23.828720Z",
     "start_time": "2021-03-03T09:20:13.331187Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标签编码\n",
    "le = LabelEncoder()\n",
    "data['api'] = le.fit_transform(data['api'])\n",
    "\n",
    "# 数据集拆分\n",
    "train_data = data[data['label'].notnull()]\n",
    "test_data = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建w2v特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:28:43.104858Z",
     "start_time": "2021-03-03T09:21:23.830809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4540d7eca044fc82da0fa4511003ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='embedding转化进度：', max=301.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creat_w2v_feature(tem_df):\n",
    "    #################################################\n",
    "    # 对api进行排序\n",
    "    tem_df = tem_df.sort_values(['api','tid','index'])\n",
    "    tem_df['api'] = tem_df['api'].astype(str) # 转化为str\n",
    "#     tem_df['api'] = le.inverse_transform(tem_df['api']) # 转化回文字\n",
    "\n",
    "    # 设置进度条\n",
    "    from tqdm.notebook import tqdm\n",
    "    tqdm.pandas(desc=\"句子转化进度：\")\n",
    "\n",
    "    # 生成api句子(去重复)\n",
    "    file_api_sequence = tem_df.groupby('file_id')['api'].apply(lambda x:x.drop_duplicates().tolist())\n",
    "\n",
    "    #################################################\n",
    "    # 进行w2v的训练\n",
    "    model = word2vec.Word2Vec(sentences=file_api_sequence, size=5, window=3, min_count=1)\n",
    "\n",
    "    # 将api信息转化为embedding\n",
    "    tqdm.pandas(desc=\"embedding转化进度：\")\n",
    "    tem_df = tem_df['api'].drop_duplicates().reset_index(drop=True).to_frame() # 去重复\n",
    "    w2v_df = tem_df['api'].progress_apply(lambda x:model.wv[x])\n",
    "    w2v_df = pd.DataFrame(w2v_df.tolist())\n",
    "    w2v_df.columns = [f'api_embedding_{i}' for i in w2v_df.columns]\n",
    "    \n",
    "    #################################################\n",
    "    # 合并表格\n",
    "    tem_df['api'] = tem_df['api'].astype(int) # 转化回INT\n",
    "    w2v_df = pd.concat([tem_df,w2v_df],axis=1) \n",
    "     \n",
    "    \n",
    "    return(w2v_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 数据集合并\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "# 进行标准化\n",
    "w2v_df = creat_w2v_feature(data)\n",
    "\n",
    "train_data = train_data.merge(w2v_df,on='api',how='left')\n",
    "test_data = test_data.merge(w2v_df,on='api',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del w2v_df,data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建病毒api类型与用户当前api的匹配程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:32:06.665191Z",
     "start_time": "2021-03-03T09:28:43.107457Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 可以优化的地方\n",
    "# embedding名称获取\n",
    "# 进行加权修正\n",
    "\n",
    "\n",
    "# 构建病毒api类型与用户当前api的匹配程度\n",
    "def create_api_virus_sim(train_data):\n",
    "    tem_df = train_data\n",
    "    # 提取每一个病毒的embedding偏好\n",
    "    virus_eb = tem_df.groupby(['label'])['api_embedding_0','api_embedding_1','api_embedding_2','api_embedding_3','api_embedding_4'].mean()\n",
    "    # 更改名称\n",
    "    virus_eb.columns = [f'virus_{i}' for i in virus_eb.columns]\n",
    "\n",
    "    # 提取file所有api的embedding\n",
    "    user_eb_feature = ['file_id','api','api_embedding_0',\n",
    "           'api_embedding_1', 'api_embedding_2', 'api_embedding_3',\n",
    "           'api_embedding_4']\n",
    "    user_eb = tem_df[user_eb_feature].set_index(['file_id','api'])\n",
    "\n",
    "    # 病毒0-7的匹配度\n",
    "    api_virus_list = []\n",
    "    for label in range(virus_eb.shape[0]):\n",
    "        virus_embedding = virus_eb.iloc[label].to_frame().T\n",
    "\n",
    "        # 进行余弦相似度的比较\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 生成相似矩阵\n",
    "        similary_array = cosine_similarity(user_eb,virus_embedding)\n",
    "\n",
    "        # 生成相似DataFrame,熟悉的中文化\n",
    "        columns_list = index_list = user_eb.index\n",
    "        similary_df = pd.DataFrame(similary_array,index=index_list,columns=[f'api_virus_sim_eb{label}'])\n",
    "\n",
    "        api_virus_list.append(similary_df)\n",
    "\n",
    "    api_virus_df = pd.concat(api_virus_list,axis=1)\n",
    "\n",
    "    # 去重，只输出api与病毒的相似度\n",
    "    api_virus_df = api_virus_df.reset_index().drop('file_id',axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    return api_virus_df\n",
    "\n",
    "\n",
    "# 通过训练集构建病毒、api的相似程度\n",
    "api_virus_df = create_api_virus_sim(train_data)\n",
    "\n",
    "# 与训练集拼接\n",
    "train_data = train_data.merge(api_virus_df,on='api',how='left')\n",
    "\n",
    "# 与测试集拼接\n",
    "test_data = test_data.merge(api_virus_df,on='api',how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成特征文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:33:08.603968Z",
     "start_time": "2021-03-03T09:32:06.667930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>api</th>\n",
       "      <th>tid</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8065</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89620181</th>\n",
       "      <td>13883</td>\n",
       "      <td>2.0</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89798402</th>\n",
       "      <td>13884</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89799721</th>\n",
       "      <td>13885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151</td>\n",
       "      <td>2240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89800754</th>\n",
       "      <td>13886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95</td>\n",
       "      <td>2324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89806070</th>\n",
       "      <td>13887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135</td>\n",
       "      <td>2336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13887 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  label  api   tid  index\n",
       "0               1    5.0  135  2488      0\n",
       "6786            2    2.0   95  2320      0\n",
       "7602            3    0.0  151  2208      0\n",
       "8065            4    0.0   95  2284      0\n",
       "10111           5    0.0  249  2500      0\n",
       "...           ...    ...  ...   ...    ...\n",
       "89620181    13883    2.0   95   100      0\n",
       "89798402    13884    5.0   95  2592      0\n",
       "89799721    13885    0.0  151  2240      0\n",
       "89800754    13886    1.0   95  2324      0\n",
       "89806070    13887    2.0  135  2336      0\n",
       "\n",
       "[13887 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_feature = ['file_id','label','api','tid','index']\n",
    "\n",
    "train_fe = train_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "test_fe = test_data[basic_feature].drop_duplicates(['file_id','label'])\n",
    "train_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建统计值特征（count,nunique,min、max、sum、mean、median）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:15.825716Z",
     "start_time": "2021-03-03T09:39:15.470933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到统计值特征（count,nunique,min、max、sum、mean、median）\n",
    "def create_stats_feature(tem_df):\n",
    "    # 以file_id 为单位聚合，统计每一个file_id下的情况\n",
    "    group_df = tem_df.groupby('file_id')\n",
    "\n",
    "    # 得到 count,nunique,min、max、sum、mean、median的统计值\n",
    "    stats_dict = {\n",
    "                'api': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'tid': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'index': ['count', 'nunique', 'min', 'max', 'mean', 'median','std'],\n",
    "                'api_embedding_0': ['mean','sum','max','min','median'],\n",
    "                'api_embedding_1': ['mean','sum','max','min','median'],\n",
    "                'api_embedding_2': ['mean','sum','max','min','median'],\n",
    "                'api_embedding_3': ['mean','sum','max','min','median'],\n",
    "                'api_embedding_4': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb1': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb2': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb3': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb4': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb5': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb6': ['mean','sum','max','min','median'],\n",
    "                'api_virus_sim_eb7': ['mean','sum','max','min','median']\n",
    "    }\n",
    "        \n",
    "  \n",
    "    stats_df = group_df.agg(stats_dict)\n",
    "    stats_df.columns = [\"_\".join(tup) for tup in stats_df.columns]\n",
    "    stats_df = stats_df.reset_index()\n",
    "    return(stats_df)\n",
    "\n",
    "\n",
    "\n",
    "# 构建训练集的特征\n",
    "stats_df = create_stats_feature(train_data)\n",
    "train_fe = train_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "# 构建测试集的特征\n",
    "stats_df = create_stats_feature(test_data)\n",
    "test_fe = test_fe.merge(stats_df,on='file_id',how='left')\n",
    "\n",
    "\n",
    "# 回收内存\n",
    "del stats_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:16.278572Z",
     "start_time": "2021-03-03T09:43:15.827547Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集合并\n",
    "data = pd.concat([train_fe,test_fe],axis=0)\n",
    "\n",
    "\n",
    "# 进行标准化\n",
    "de_ss_list = ['file_id','label']\n",
    "ss_list = [i for i in data.columns if i not in de_ss_list]\n",
    "\n",
    "ss = StandardScaler()\n",
    "data[ss_list] = ss.fit_transform(data[ss_list])\n",
    "\n",
    "\n",
    "# 数据集拆分\n",
    "train_fe = data[data['label'].notnull()]\n",
    "test_fe = data[data['label'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:16.324356Z",
     "start_time": "2021-03-03T09:43:16.280875Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe.to_pickle(f'{path}/train_fe_v3.pkl')\n",
    "test_fe.to_pickle(f'{path}/test_fe_v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:16.343866Z",
     "start_time": "2021-03-03T09:43:16.326687Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./temdata\"\n",
    "train_fe = pd.read_pickle(f'{path}/train_fe_v3.pkl')\n",
    "test_fe = pd.read_pickle(f'{path}/test_fe_v3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:16.352614Z",
     "start_time": "2021-03-03T09:43:16.346256Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 进行数据切分\n",
    "# file_id_list = train_fe['file_id'].unique()\n",
    "\n",
    "# train_id,vaild_id = train_test_split(file_id_list,test_size=0.2)\n",
    "# train_df = train_fe[train_fe['file_id'].isin(train_id)]\n",
    "# vaild_df = train_fe[train_fe['file_id'].isin(vaild_id)]\n",
    "\n",
    "\n",
    "# # 得到训练数据集与提交数据集\n",
    "# train_x,train_y = train_df.drop('label',axis=1),train_df['label']\n",
    "# vaild_x,vaild_y = vaild_df.drop('label',axis=1),vaild_df['label']\n",
    "# sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:16.373167Z",
     "start_time": "2021-03-03T09:43:16.354263Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据划分\n",
    "raw_x,raw_y = train_fe.drop('label',axis=1),train_fe['label']\n",
    "sub_x,sub_y = test_fe.drop('label',axis=1),test_fe['label']\n",
    "\n",
    "# 进行数据切分\n",
    "train_x,vaild_x,train_y,vaild_y = train_test_split(raw_x,raw_y,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:43:16.377279Z",
     "start_time": "2021-03-03T09:43:16.375060Z"
    }
   },
   "outputs": [],
   "source": [
    "# columns_list = ['api_count', 'api_nunique', 'api_mean', 'api_sum',\n",
    "#        'api_max', 'api_min', 'api_median', 'tid_count', 'tid_nunique',\n",
    "#        'tid_mean', 'tid_sum', 'tid_max', 'tid_min', 'tid_median',\n",
    "#        'index_count', 'index_nunique', 'index_mean', 'index_sum', 'index_max',\n",
    "#        'index_min', 'index_median']\n",
    "# train_x = train_x[columns_list]\n",
    "# vaild_x = vaild_x[columns_list]\n",
    "# sub_x = sub_x[columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模尝试"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T05:48:29.123292Z",
     "start_time": "2021-03-01T05:48:29.119064Z"
    }
   },
   "source": [
    "模型思路：（保存到竞赛文档，用于之后快速构建baseline）\n",
    "* lgb祖传参数 （0.73）\n",
    "* xgb祖传参数 （0.68）\n",
    "* nn直桶网络\n",
    "* 简单加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:46:09.718687Z",
     "start_time": "2021-03-03T09:43:16.379581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=1, learning_rate=0.005, min_child_samples=3,\n",
       "               n_estimators=2000, objective='multiclass', random_state=2021,\n",
       "               reg_alpha=0.25, reg_lambda=0.25, subsample=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/lgb_v3.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0}\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "            num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='multiclass',\n",
    "            max_depth=-1, learning_rate=0.005, min_child_samples=3, random_state=2021,\n",
    "            n_estimators=2000, subsample=1, colsample_bytree=1,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='logloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/lgb_v3.csv',index=None)\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/lgb_v3.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('lgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用xgb进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:51:15.711277Z",
     "start_time": "2021-03-03T09:46:09.720661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:09] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { min_child_samples } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, eval_metric='mlogloss',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.005, max_delta_step=0,\n",
       "              max_depth=9, min_child_samples=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=2000, n_jobs=0,\n",
       "              num_parallel_tree=1, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=0.5, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['./modelfile/xgb_v3.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存完毕！\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "gpu=False\n",
    "\n",
    "#############################################\n",
    "\n",
    "if gpu==True:\n",
    "    params = {'tree_method': 'gpu_hist'}\n",
    "\n",
    "else:\n",
    "    params = {}\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "            max_depth=9, learning_rate=0.005, n_estimators=2000, \n",
    "            objective='multi:softprob', \n",
    "            subsample=0.8, colsample_bytree=0.8, \n",
    "            min_child_samples=3, eval_metric='mlogloss', reg_lambda=0.5,**params)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 模型训练\n",
    "# model.fit(\n",
    "#     train_x, train_y,\n",
    "#     eval_metric='mlogloss', eval_set=[(train_x, train_y), (vaild_x, vaild_y)],\n",
    "#     verbose=100,\n",
    "#     #早停法，如果auc在10epoch没有进步就stop\n",
    "#     early_stopping_rounds=1000 )\n",
    "\n",
    "model.fit(raw_x,raw_y)\n",
    "\n",
    "\n",
    "\n",
    "# 预测\n",
    "pred_prob = model.predict_proba(sub_x)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "sub_df.to_csv(f'{path}/xgb_v3.csv',index=None)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 保存模型\n",
    "import joblib\n",
    "# 模型存储\n",
    "joblib.dump(model, './modelfile/xgb_v3.pkl')\n",
    "# 模型加载\n",
    "# model = joblib.load('xgb.pkl')\n",
    "print('模型保存完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用神经网络进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:33:08.945748Z",
     "start_time": "2021-03-03T09:19:13.659Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,RemoteMonitor,CSVLogger\n",
    "\n",
    "#############################################\n",
    "# 数据格式转化\n",
    "\n",
    "cate_train_y = keras.utils.to_categorical(train_y,8)\n",
    "cate_vaild_y = keras.utils.to_categorical(vaild_y,8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "# 搭建模型\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation='relu', input_shape=[len(train_x.columns)]),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'), \n",
    "    keras.layers.Dense(8, activation='softmax') # 需要改写成sigmoid\n",
    "])\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 配置损失函数，评估指标，优化器\n",
    "model.compile(loss='categorical_crossentropy', metrics=['categorical_crossentropy'], optimizer='adam')\n",
    "\n",
    "\n",
    "# 配置backcall\n",
    "callback = ModelCheckpoint(filepath=\"modelfile/MLP_v3.ckpt\",\n",
    "                           monitor=\"val_categorical_crossentropy\",\n",
    "                           verbose=1,save_best_only=True,\n",
    "                           save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:33:08.946755Z",
     "start_time": "2021-03-03T09:19:13.661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#############################################\n",
    "# 训练\n",
    "model.fit(train_x, cate_train_y, \n",
    "          validation_data=(vaild_x, cate_vaild_y), \n",
    "          batch_size=100, epochs=5,\n",
    "          callbacks=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:33:08.947657Z",
     "start_time": "2021-03-03T09:19:13.662Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 进行预测\n",
    "pred_prob = model.predict(sub_x)\n",
    "\n",
    "#############################################\n",
    "# 转化为提交文件\n",
    "# 转化为DataFrame\n",
    "sub_df = pd.DataFrame(pred_prob)\n",
    "sub_df.insert(0,\"file_id\",sub_x['file_id'].reset_index(drop=True))\n",
    "\n",
    "# 更改列名\n",
    "sub_df.columns = list(map(lambda x:f\"prob{x}\" if x!='file_id' else x,sub_df.columns))\n",
    "sub_df = sub_df.astype('double')\n",
    "sub_df['file_id'] = sub_df['file_id'].astype(int)\n",
    "\n",
    "# 保存在本地\n",
    "path = './outdata'\n",
    "sub_df.to_csv(f'{path}/mlp_v3.csv',index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通的加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T09:33:08.948559Z",
     "start_time": "2021-03-03T09:19:13.817Z"
    }
   },
   "outputs": [],
   "source": [
    "# 打开之前的预测结果proba\n",
    "path = './outdata'\n",
    "lgb_pred = pd.read_csv(f'{path}/lgb_v3.csv').set_index('file_id')\n",
    "xgb_pred = pd.read_csv(f'{path}/xgb_v3.csv').set_index('file_id')\n",
    "mlp_pred = pd.read_csv(f'{path}/mlp_v3.csv').set_index('file_id')\n",
    "\n",
    "# 设置权重\n",
    "lgb_weight = 0.2\n",
    "xgb_weight = 0.7\n",
    "mlp_weight = 0.1\n",
    "\n",
    "# 进行加权融合\n",
    "mix_pred = (lgb_weight*lgb_pred)+(xgb_weight*xgb_pred)+(mlp_weight*mlp_pred)\n",
    "\n",
    "# 保存加权融合后的结果\n",
    "mix_pred.to_csv(f'{path}/mix_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
